# /DE 4 - Delta Live Tables/DE 4.0 - Module Introduction
<hr>--i18n-05f37e48-e8d1-4b0c-87e8-38cd4c42edc6
## Criando pipelines de dados com o Delta Live Tables
Este módulo faz parte do Caminho de aprendizagem do engenheiro de dados da Databricks Academy.
# MAGIC
#### IU do DLT
# MAGIC
Palestra: A arquitetura medallion <br>
Palestra: Introdução ao Delta Live Tables <br>
[DE 4.1 - Usando a IU do DLT]($./DE 4.1 - Passo a passo da IU do DLT) <br>
# MAGIC
#### Sintaxe DLT
DE 4.1.1 - Pipeline de pedidos: [SQL]($./DE 4.1A - Pipelines SQL/DE 4.1.1 - Pipeline de pedidos) ou [Python]($./DE 4.1B - Pipelines Python/DE 4.1.1 - Pipeline de pedidos)<br>
DE 4.1.2 - Pipeline de clientes: [SQL]($./DE 4.1A - Pipelines SQL/DE 4.1.2 - Pipeline de clientes) ou [Python]($./DE 4.1B - Pipelines Python/DE 4.1.2 - Pipeline de clientes) <br>
[DE 4.2 - Python versus SQL]($./DE 4.2 - Python versus SQL) <br>
# MAGIC
#### Resultados, monitoramento e solução de problemas do pipeline
[DE 4.3 - Resultados do pipeline]($./DE 4.3 - Resultados do pipeline) <br>
[DE 4.4 - Logs de eventos de pipeline]($./DE 4.4 - Logs de eventos de pipeline) <br>
DE 4.1.3 - Pipeline de status: [SQL]($./DE 4.1A - Pipelines SQL/DE 4.1.3 - Pipeline de status) ou [Python]($./DE 4.1B - Pipelines Python/DE 4.1.3 - Pipeline de status) <br>
[DE 4.99 - Obter novos dados]($./DE 4.99 - Obter novos dados) <br>
# MAGIC
#### Pré-requisitos
# MAGIC
*Familiaridade para iniciantes com conceitos de computação em nuvem (máquinas virtuais, armazenamento de objetos, etc.)
* Capacidade de executar tarefas básicas de desenvolvimento de código usando o workspace de ciência de dados e engenharia do Databricks (criar clusters, executar código em notebooks, realizar operações básicas em notebooks, importar repos do Git etc.)
* Experiência inicial de programação com o Delta Lake
* Usar a DLL do Delta Lake para criar tabelas, compactar arquivos, restaurar versões anteriores de tabelas e realizar a coleta de lixo em tabelas no Lakehouse
  * Usar CTAS para armazenar dados derivados de uma query em uma tabela do Delta Lake
  * Usar SQL para realizar atualizações completas e incrementais em tabelas existentes
* Experiência inicial de programação com Spark SQL ou PySpark
  *Extraia dados de vários formatos de arquivo e fontes de dados
  *Aplique uma série de transformações comuns para limpar dados
  *Remodele e manipule dados complexos usando recursos avançados- em funções
* Experiência de produção trabalhando com data warehouses e data lakes
# MAGIC
#### Considerações técnicas
# MAGIC
*Este curso é executado em DBR 11.3.
*Este curso não pode ser ministrado no Databricks Community Edition.
