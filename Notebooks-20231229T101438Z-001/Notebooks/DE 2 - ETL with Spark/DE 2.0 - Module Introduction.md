# /DE 2 - ETL with Spark/DE 2.0 - Module Introduction
<hr>--i18n-2fbad123-4065-4749-a925-d24f111ab27c
## Transformar dados com o Spark
Este módulo faz parte do roteiro de aprendizagem para engenheiros de dados da Databricks Academy e pode ser feito em SQL ou Python.
# MAGIC
#### Extraindo dados
Estes notebooks demonstram conceitos do Spark SQL relevantes para usuários de SQL e PySpark.  
# MAGIC
[DE 2.1 - Consultando arquivos diretamente]($./DE 2.1 - Consultando arquivos diretamente)  
[DE 2.2 - Fornecendo opções para fontes externas]($./DE 2.2 - Fornecendo opções para fontes externas)  
[DE 2.3L - Laboratório de extração de dados]($./DE 2.3L - Laboratório de extração de dados)
# MAGIC
#### Transformando dados
Estes notebooks incluem queries Spark SQL ao lado de código PySpark do DataFrame para demonstrar os mesmos conceitos em ambas as linguagens.
# MAGIC
[DE 2.4 - Limpando dados]($./DE 2.4 - Limpando dados)  
[DE 2.5 - Transformações complexas]($./DE 2.5 - Transformações complexas)  
[DE 2.6L - Laboratório de remodelagem de dados]($./DE 2.6L - Laboratório de remodelagem de dados)
# MAGIC
#### Funções adicionais
# MAGIC
[DE 2.7A - UDFs SQL]($./DE 2.7A - UDFs SQL)  
[DE 2.7B - UDFs Python]($./DE 2.7B - UDFs Python)  
[DE 2.99 - Funções de ordem superior – OPCIONAIS]($./DE 2.99 - Funções de ordem superior – OPCIONAIS)  
# MAGIC
### Pré-requisitos
Pré-requisitos para as duas versões deste curso (Spark SQL e PySpark):
* Familiaridade elementar com conceitos básicos de nuvem (máquinas virtuais, armazenamento de objetos, gerenciamento de identidades)
* Capacidade de executar tarefas básicas de desenvolvimento de código usando o workspace de ciência de dados e engenharia do Databricks (criar clusters, executar código em notebooks, realizar operações básicas em notebooks, importar repos do git etc.)
* Familiaridade intermediária com conceitos básicos de SQL (select, filter, groupby, join etc)
# MAGIC
Pré-requisitos adicionais para a versão PySpark deste curso:
* Experiência inicial em programação com Python (sintaxe, condições, loops, funções)
* Experiência inicial em programação com a API DataFrame do Spark:
* Configurar o DataFrameReader e o DataFrameWriter para ler e gravar dados
* Expressar transformações de query usando métodos do DataFrame e expressões de coluna
* Navegar na documentação do Spark para identificar funções integradas para várias transformações e tipos de dados
# MAGIC
Os alunos podem fazer o curso Introdução à programação PySpark da Databricks Academy para aprender as habilidades prévias de programação com a API DataFrame do Spark. <br>
# MAGIC
# MAGIC
#### Considerações técnicas
* Este curso é executado no DBR 11.3.
*Este curso não pode ser ministrado no Databricks Community Edition.
